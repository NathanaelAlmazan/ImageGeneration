{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchsummary import summary\n",
    "from fastprogress import progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define image transformation\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((128, 128)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# download dataset\n",
    "train_dataset = torchvision.datasets.SUN397(root='dataset', transform=transform, download=True)\n",
    "\n",
    "# # select classes\n",
    "classes_idx = [idx for idx, label in enumerate(train_dataset.classes) if label in\n",
    "    ['bedroom', 'beach', 'skyscraper', 'lighthouse', 'windmill', 'mountain', 'castle', 'rice_paddy', 'forest_path', 'bridge']]\n",
    "\n",
    "# filter dataset by classes\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, [idx for idx, label in enumerate(train_dataset._labels) if label in classes_idx])\n",
    "\n",
    "print('Dataset size:', len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# extract a batch of images\n",
    "batch_images, batch_labels = next(iter(train_loader))\n",
    "# display a grid of images\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.imshow(np.transpose(torchvision.utils.make_grid(batch_images[:8], padding=0).numpy(), (1, 2, 0)))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.group_norm_1 = torch.nn.GroupNorm(32, in_channels)\n",
    "        self.conv_1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.group_norm_2 = torch.nn.GroupNorm(32, out_channels)\n",
    "        self.conv_2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        if in_channels == out_channels:\n",
    "            self.residual_layer = torch.nn.Identity()\n",
    "        else:\n",
    "            self.residual_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residue = x\n",
    "\n",
    "        x = self.group_norm_1(x)\n",
    "        x = torch.nn.functional.silu(x)\n",
    "        x = self.conv_1(x)\n",
    "\n",
    "        x = self.group_norm_2(x)\n",
    "        x = torch.nn.functional.silu(x)\n",
    "        x = self.conv_2(x)\n",
    "\n",
    "        return x + self.residual_layer(residue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, n_heads, d_embed, in_proj_bias=True, out_proj_bias=True):\n",
    "        super().__init__()\n",
    "        self.in_proj = torch.nn.Linear(d_embed, 3 * d_embed, bias=in_proj_bias)\n",
    "        self.out_proj = torch.nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_embed // n_heads\n",
    "\n",
    "    def forward(self, x, causal_mask=False):\n",
    "        input_shape = x.shape\n",
    "        batch_size, sequence_length, _ = input_shape\n",
    "        interim_shape = (batch_size, sequence_length, self.n_heads, self.d_head)\n",
    "\n",
    "        q, k, v = self.in_proj(x).chunk(3, dim=-1)\n",
    "\n",
    "        q = q.view(interim_shape).transpose(1, 2)\n",
    "        k = k.view(interim_shape).transpose(1, 2)\n",
    "        v = v.view(interim_shape).transpose(1, 2)\n",
    "\n",
    "        weight = q @ k.transpose(-1, -2)\n",
    "        if causal_mask:\n",
    "            mask = torch.ones_like(weight, dtype=torch.bool).triu(1)\n",
    "            weight.masked_fill_(mask, -torch.inf)\n",
    "        weight /= math.sqrt(self.d_head)\n",
    "        weight = torch.nn.functional.softmax(weight, dim=-1)\n",
    "\n",
    "        output = weight @ v\n",
    "        output = output.transpose(1, 2)\n",
    "        output = output.reshape(input_shape)\n",
    "        output = self.out_proj(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.group_norm = torch.nn.GroupNorm(32, channels)\n",
    "        self.attention = SelfAttention(1, channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residue = x\n",
    "        x = self.group_norm(x)\n",
    "\n",
    "        n, c, h, w = x.shape\n",
    "        x = x.view((n, c, h * w))\n",
    "        x = x.transpose(-1, -2)\n",
    "        x = self.attention(x)\n",
    "        x = x.transpose(-1, -2)\n",
    "        x = x.view((n, c, h, w))\n",
    "\n",
    "        x += residue\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            torch.nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            ResidualBlock(64, 64),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=0),\n",
    "            ResidualBlock(64, 128),\n",
    "            torch.nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=0),\n",
    "            ResidualBlock(128, 256),\n",
    "            torch.nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=0),\n",
    "            ResidualBlock(256, 256),\n",
    "            AttentionBlock(256),\n",
    "            ResidualBlock(256, 256),\n",
    "            torch.nn.GroupNorm(32, 256),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Conv2d(256, 8, kernel_size=3, padding=1),\n",
    "            torch.nn.Conv2d(8, 8, kernel_size=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self:\n",
    "            if getattr(module, 'stride', None) == (2, 2):\n",
    "                x = torch.nn.functional.pad(x, (0, 1, 0, 1))\n",
    "            x = module(x)\n",
    "\n",
    "        mean, log_variance = torch.chunk(x, 2, dim=1)\n",
    "        log_variance = torch.clamp(log_variance, -30, 20)\n",
    "        variance = log_variance.exp()\n",
    "        stdev = variance.sqrt()\n",
    "        noise = torch.randn(*mean.size()).float().cuda()\n",
    "        x = mean + stdev * noise\n",
    "\n",
    "        x *= 0.18215\n",
    "        return mean, log_variance, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent space shape torch.Size([8, 4, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder().float().cuda()\n",
    "z_mean, z_log_var, z = encoder(torch.rand((8, 3, 128, 128)).float().cuda())\n",
    "\n",
    "print('Latent space shape', z_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           1,792\n",
      "         GroupNorm-2         [-1, 64, 128, 128]             128\n",
      "            Conv2d-3         [-1, 64, 128, 128]          36,928\n",
      "         GroupNorm-4         [-1, 64, 128, 128]             128\n",
      "            Conv2d-5         [-1, 64, 128, 128]          36,928\n",
      "          Identity-6         [-1, 64, 128, 128]               0\n",
      "     ResidualBlock-7         [-1, 64, 128, 128]               0\n",
      "            Conv2d-8           [-1, 64, 64, 64]          36,928\n",
      "         GroupNorm-9           [-1, 64, 64, 64]             128\n",
      "           Conv2d-10          [-1, 128, 64, 64]          73,856\n",
      "        GroupNorm-11          [-1, 128, 64, 64]             256\n",
      "           Conv2d-12          [-1, 128, 64, 64]         147,584\n",
      "           Conv2d-13          [-1, 128, 64, 64]           8,320\n",
      "    ResidualBlock-14          [-1, 128, 64, 64]               0\n",
      "           Conv2d-15          [-1, 128, 32, 32]         147,584\n",
      "        GroupNorm-16          [-1, 128, 32, 32]             256\n",
      "           Conv2d-17          [-1, 256, 32, 32]         295,168\n",
      "        GroupNorm-18          [-1, 256, 32, 32]             512\n",
      "           Conv2d-19          [-1, 256, 32, 32]         590,080\n",
      "           Conv2d-20          [-1, 256, 32, 32]          33,024\n",
      "    ResidualBlock-21          [-1, 256, 32, 32]               0\n",
      "           Conv2d-22          [-1, 256, 16, 16]         590,080\n",
      "        GroupNorm-23          [-1, 256, 16, 16]             512\n",
      "           Conv2d-24          [-1, 256, 16, 16]         590,080\n",
      "        GroupNorm-25          [-1, 256, 16, 16]             512\n",
      "           Conv2d-26          [-1, 256, 16, 16]         590,080\n",
      "         Identity-27          [-1, 256, 16, 16]               0\n",
      "    ResidualBlock-28          [-1, 256, 16, 16]               0\n",
      "        GroupNorm-29          [-1, 256, 16, 16]             512\n",
      "           Linear-30             [-1, 256, 768]         197,376\n",
      "           Linear-31             [-1, 256, 256]          65,792\n",
      "    SelfAttention-32             [-1, 256, 256]               0\n",
      "   AttentionBlock-33          [-1, 256, 16, 16]               0\n",
      "        GroupNorm-34          [-1, 256, 16, 16]             512\n",
      "           Conv2d-35          [-1, 256, 16, 16]         590,080\n",
      "        GroupNorm-36          [-1, 256, 16, 16]             512\n",
      "           Conv2d-37          [-1, 256, 16, 16]         590,080\n",
      "         Identity-38          [-1, 256, 16, 16]               0\n",
      "    ResidualBlock-39          [-1, 256, 16, 16]               0\n",
      "        GroupNorm-40          [-1, 256, 16, 16]             512\n",
      "             SiLU-41          [-1, 256, 16, 16]               0\n",
      "           Conv2d-42            [-1, 8, 16, 16]          18,440\n",
      "           Conv2d-43            [-1, 8, 16, 16]              72\n",
      "================================================================\n",
      "Total params: 4,644,752\n",
      "Trainable params: 4,644,752\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 103.03\n",
      "Params size (MB): 17.72\n",
      "Estimated Total Size (MB): 120.94\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(encoder, input_size=(3, 128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            torch.nn.Conv2d(4, 4, kernel_size=1, padding=0),\n",
    "            torch.nn.Conv2d(4, 256, kernel_size=3, padding=1),\n",
    "            ResidualBlock(256, 256),\n",
    "            AttentionBlock(256),\n",
    "            ResidualBlock(256, 256),\n",
    "            torch.nn.Upsample(scale_factor=2),\n",
    "            torch.nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            ResidualBlock(256, 256),\n",
    "            torch.nn.Upsample(scale_factor=2),\n",
    "            torch.nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            ResidualBlock(256, 128),\n",
    "            torch.nn.Upsample(scale_factor=2),\n",
    "            torch.nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            ResidualBlock(128, 64),\n",
    "            torch.nn.GroupNorm(32, 64),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x /= 0.18215\n",
    "        for module in self:\n",
    "            x = module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction shape torch.Size([8, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder().float().cuda()\n",
    "reconstruction = decoder(torch.rand((8, 4, 16, 16)).float().cuda())\n",
    "\n",
    "print('Reconstruction shape', reconstruction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 4, 16, 16]              20\n",
      "            Conv2d-2          [-1, 256, 16, 16]           9,472\n",
      "         GroupNorm-3          [-1, 256, 16, 16]             512\n",
      "            Conv2d-4          [-1, 256, 16, 16]         590,080\n",
      "         GroupNorm-5          [-1, 256, 16, 16]             512\n",
      "            Conv2d-6          [-1, 256, 16, 16]         590,080\n",
      "          Identity-7          [-1, 256, 16, 16]               0\n",
      "     ResidualBlock-8          [-1, 256, 16, 16]               0\n",
      "         GroupNorm-9          [-1, 256, 16, 16]             512\n",
      "           Linear-10             [-1, 256, 768]         197,376\n",
      "           Linear-11             [-1, 256, 256]          65,792\n",
      "    SelfAttention-12             [-1, 256, 256]               0\n",
      "   AttentionBlock-13          [-1, 256, 16, 16]               0\n",
      "        GroupNorm-14          [-1, 256, 16, 16]             512\n",
      "           Conv2d-15          [-1, 256, 16, 16]         590,080\n",
      "        GroupNorm-16          [-1, 256, 16, 16]             512\n",
      "           Conv2d-17          [-1, 256, 16, 16]         590,080\n",
      "         Identity-18          [-1, 256, 16, 16]               0\n",
      "    ResidualBlock-19          [-1, 256, 16, 16]               0\n",
      "         Upsample-20          [-1, 256, 32, 32]               0\n",
      "           Conv2d-21          [-1, 256, 32, 32]         590,080\n",
      "        GroupNorm-22          [-1, 256, 32, 32]             512\n",
      "           Conv2d-23          [-1, 256, 32, 32]         590,080\n",
      "        GroupNorm-24          [-1, 256, 32, 32]             512\n",
      "           Conv2d-25          [-1, 256, 32, 32]         590,080\n",
      "         Identity-26          [-1, 256, 32, 32]               0\n",
      "    ResidualBlock-27          [-1, 256, 32, 32]               0\n",
      "         Upsample-28          [-1, 256, 64, 64]               0\n",
      "           Conv2d-29          [-1, 256, 64, 64]         590,080\n",
      "        GroupNorm-30          [-1, 256, 64, 64]             512\n",
      "           Conv2d-31          [-1, 128, 64, 64]         295,040\n",
      "        GroupNorm-32          [-1, 128, 64, 64]             256\n",
      "           Conv2d-33          [-1, 128, 64, 64]         147,584\n",
      "           Conv2d-34          [-1, 128, 64, 64]          32,896\n",
      "    ResidualBlock-35          [-1, 128, 64, 64]               0\n",
      "         Upsample-36        [-1, 128, 128, 128]               0\n",
      "           Conv2d-37        [-1, 128, 128, 128]         147,584\n",
      "        GroupNorm-38        [-1, 128, 128, 128]             256\n",
      "           Conv2d-39         [-1, 64, 128, 128]          73,792\n",
      "        GroupNorm-40         [-1, 64, 128, 128]             128\n",
      "           Conv2d-41         [-1, 64, 128, 128]          36,928\n",
      "           Conv2d-42         [-1, 64, 128, 128]           8,256\n",
      "    ResidualBlock-43         [-1, 64, 128, 128]               0\n",
      "        GroupNorm-44         [-1, 64, 128, 128]             128\n",
      "             SiLU-45         [-1, 64, 128, 128]               0\n",
      "           Conv2d-46          [-1, 3, 128, 128]           1,731\n",
      "================================================================\n",
      "Total params: 5,741,975\n",
      "Trainable params: 5,741,975\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 174.38\n",
      "Params size (MB): 21.90\n",
      "Estimated Total Size (MB): 196.29\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(decoder, input_size=(4, 16, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Variational Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "LEARNING_RATE = 5e-3\n",
    "\n",
    "def loss_fn(recon_x, x, mean, log_var):\n",
    "    mse = torch.nn.functional.mse_loss(recon_x, x, size_average=False)\n",
    "    kld = -0.5 * torch.mean(1 + log_var - torch.pow(mean, 2) - torch.exp(log_var))\n",
    "\n",
    "    return mse + kld, mse, kld\n",
    "\n",
    "def save_checkpoint(checkpoint_path=\"diffusion_vae\"):\n",
    "    torch.save(encoder.state_dict(), os.path.join(checkpoint_path, \"encoder_ckpt.pt\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(checkpoint_path, \"decoder_ckpt.pt\"))\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "optimizer = torch.optim.AdamW(list(encoder.parameters()) + list(decoder.parameters()), lr=LEARNING_RATE, eps=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    p_bar = progress_bar(train_loader, leave=False)\n",
    "    avg_loss = 0.\n",
    "    avg_kl_loss = 0.\n",
    "    avg_reconstruction_loss = 0.\n",
    "    for image, label in p_bar:\n",
    "        with torch.autocast(\"cuda\") and torch.enable_grad():\n",
    "            # forward pass\n",
    "            z_mean, z_log_var, z = encoder(image)\n",
    "            reconstruction = decoder(z)\n",
    "            # calculate loss\n",
    "            loss, mse, kld = loss_fn(reconstruction, image, z_mean, z_log_var)\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        # logger\n",
    "        avg_loss += loss\n",
    "        avg_kl_loss += kld\n",
    "        avg_reconstruction_loss += mse\n",
    "        p_bar.comment = f\"total_loss: {loss.item():.2e}, reconstruction_loss: {mse.item():.2e}, kl_loss: {kld.item():.2e}\"\n",
    "    # log average loss\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}: total_loss: {avg_loss.mean().item():.2e}, reconstruction_loss: {avg_reconstruction_loss.mean().item():.2e}, kl_loss: {avg_kl_loss.mean().item():.2e}\")\n",
    "    # save checkpoint\n",
    "    save_checkpoint(\"diffusion_vae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(\"diffusion_vae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_state_dict(torch.load(os.path.join(\"diffusion_vae\", \"encoder_ckpt.pt\")))\n",
    "decoder.load_state_dict(torch.load(os.path.join(\"diffusion_vae\", \"decoder_ckpt.pt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, z_log_var, z = encoder(batch_images)\n",
    "reconstructed = decoder(z)\n",
    "\n",
    "# display a reconstructed images\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.imshow(np.transpose(torchvision.utils.make_grid(reconstructed[:8], padding=0).numpy(), (1, 2, 0)))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
